{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f220447b-2c4a-4ddc-979b-8f69e6ebbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Function to translate text using MarianMT\n",
    "def translate_text_with_marianmt(text, target_language=\"fr\"):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-en-{target_language}'\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Translate the input text to the target language\n",
    "    translated_text = model.generate(**tokenizer(text, return_tensors=\"pt\"), max_length=128, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(translated_text[0], skip_special_tokens=True)\n",
    "\n",
    "# Define the input text\n",
    "input_text = '''Art 8 • Vie privée et familiale • Refus des juridictions internes d’examiner \n",
    "l’action du requérant, affirmant être le père biologique d’un enfant, visant à \n",
    "contester la paternité légalement établie en vue de faire établir la sienne, en \n",
    "l’application des règles de computation du délai de forclusion de cinq ans \n",
    "combinées avec l’obligation d’attraire en la cause l’enfant • Requérant n’ayant \n",
    "pas agi dès la connaissance de sa paternité alors qu’il disposait d’un délai \n",
    "suffisant de plus de trois ans pour engager une action • Requérant ayant tardé à \n",
    "mettre dans la cause l’enfant sans justifier avoir pu ignorer l’existence de cette \n",
    "règle constante en droit interne • Conclusions des juridictions internes ni \n",
    "arbitraires ni déraisonnables • Refus fondé sur un lien de filiation déjà établi pour \n",
    "l’enfant et au regard de l’intérêt supérieur de ce dernier • Décisions judiciaires \n",
    "n’ayant pas abouti en pratique à priver le requérant de tout lien avec l’enfant • \n",
    "Juste équilibre ménagé entre les différents intérêts en présence'''\n",
    "\n",
    "# Translate the text to the target language (e.g., \"fr\" for French)\n",
    "translated_text = translate_text_with_marianmt(input_text, target_language=\"fr\")\n",
    "\n",
    "# Save the translated text to a file\n",
    "with open(\"translated_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad0b857-14fd-40c5-9824-a4f615f4c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17594 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m output_pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_outputBERT.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Translate and create the new PDF\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mcreate_translated_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_pdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation complete. Translated PDF saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_pdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mcreate_translated_pdf\u001b[1;34m(input_pdf_path, output_pdf_path, target_language)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_translated_pdf\u001b[39m(input_pdf_path, output_pdf_path, target_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     24\u001b[0m     extracted_text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(input_pdf_path)\n\u001b[1;32m---> 25\u001b[0m     translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text_with_marianmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     pdf_document \u001b[38;5;241m=\u001b[39m fitz\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m     28\u001b[0m     pdf_document\u001b[38;5;241m.\u001b[39minsert_page(\u001b[38;5;241m0\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mtranslate_text_with_marianmt\u001b[1;34m(text, target_language)\u001b[0m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MarianTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Translate the input text to the target language\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated_text[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1496\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1489\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1490\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1491\u001b[0m         )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1496\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    659\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 661\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:759\u001b[0m, in \u001b[0;36mMarianEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale\n\u001b[1;32m--> 759\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m embed_pos\n\u001b[0;32m    762\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:140\u001b[0m, in \u001b[0;36mMarianSinusoidalPositionalEmbedding.forward\u001b[1;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[0;32m    136\u001b[0m bsz, seq_len \u001b[38;5;241m=\u001b[39m input_ids_shape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    137\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m    138\u001b[0m     past_key_values_length, past_key_values_length \u001b[38;5;241m+\u001b[39m seq_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Input and output file paths\n",
    "input_pdf_path = \"uploads/AFFAIRE C.P. ET M.N. c. FRANCE.pdf\"\n",
    "output_pdf_path = \"translated_outputBERT.pdf\"\n",
    "\n",
    "# Translate and create the new PDF\n",
    "create_translated_pdf(input_pdf_path, output_pdf_path)\n",
    "\n",
    "print(f\"Translation complete. Translated PDF saved at: {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5bccb7-db59-409c-9e54-51b267f09478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Art. 8 • Private and family life • Refusal of domestic courts to examine the applicant's action, claiming to be the biological father of a child, to challenge the legally established paternity with a view to establishing his/her own, in application of the rules for calculating the five-year period of foreclosure combined with the obligation of attrition in the case of the child • Complainant who did not act as soon as he/she knew his/her fatherhood when he/she had sufficient time to initiate an action • Complainant who delayed bringing the child into the case without having been able to ignore the existence of this constant rule in domestic law • Findings of internal courts neither arbitrary nor unreasonable • Refusal based on a filiation link already established for the child and in view of the best interests of the child • Judicial decisions which did not in practice lead to depriving the applicant of any connection with the child • Fair balance between the various interests involved\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate_text_with_marianmt(text, target_language=\"en\"):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{target_language}-en'  # Replace \"en\" with the desired target language code\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Translate the input text to the target language\n",
    "    translation_ids = model.generate(**inputs, num_beams=4, max_length=512, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage:\n",
    "input_text = \"\"\"Art 8 • Vie privée et familiale • Refus des juridictions internes d’examiner \n",
    "l’action du requérant, affirmant être le père biologique d’un enfant, visant à \n",
    "contester la paternité légalement établie en vue de faire établir la sienne, en \n",
    "l’application des règles de computation du délai de forclusion de cinq ans \n",
    "combinées avec l’obligation d’attraire en la cause l’enfant • Requérant n’ayant \n",
    "pas agi dès la connaissance de sa paternité alors qu’il disposait d’un délai \n",
    "suffisant de plus de trois ans pour engager une action • Requérant ayant tardé à \n",
    "mettre dans la cause l’enfant sans justifier avoir pu ignorer l’existence de cette \n",
    "règle constante en droit interne • Conclusions des juridictions internes ni \n",
    "arbitraires ni déraisonnables • Refus fondé sur un lien de filiation déjà établi pour \n",
    "l’enfant et au regard de l’intérêt supérieur de ce dernier • Décisions judiciaires \n",
    "n’ayant pas abouti en pratique à priver le requérant de tout lien avec l’enfant • \n",
    "Juste équilibre ménagé entre les différents intérêts en présence\"\"\"\n",
    "target_language = \"fr\"  # Replace with the target language code, e.g., \"fr\" for French\n",
    "\n",
    "translated_text = translate_text_with_marianmt(input_text, target_language)\n",
    "print(\"Translated text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ed0199-f738-4b26-acd5-06222c0a2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize the Marian NMT model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-fr-en\"  # Replace with the model of your choice\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def translate_text(text, target_language=\"en\"):\n",
    "    # Split the text into segments of 1024 tokens or less\n",
    "    max_segment_length = 1024\n",
    "    text_segments = [text[i:i+max_segment_length] for i in range(0, len(text), max_segment_length)]\n",
    "    \n",
    "    # Translate each segment and join the results\n",
    "    translated_segments = []\n",
    "    for segment in text_segments:\n",
    "        inputs = tokenizer(segment, return_tensors=\"pt\", max_length=max_segment_length, truncation=True, padding=True)\n",
    "        translated = model.generate(**inputs, max_length=max_segment_length, num_beams=4, early_stopping=True, \n",
    "                                    length_penalty=2.0, no_repeat_ngram_size=3, num_return_sequences=1)\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translated_segments.append(translated_text[0])\n",
    "    \n",
    "    return \" \".join(translated_segments)\n",
    "\n",
    "def translate_pdf(input_pdf_path, output_txt_path, target_language=\"en\"):\n",
    "    # Open the original PDF for reading\n",
    "    pdf_reader = PdfReader(input_pdf_path)\n",
    "\n",
    "    # Create a text file for writing\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as output_txt:\n",
    "        # Iterate through pages in the original PDF\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "            translated_text = translate_text(text, target_language)\n",
    "            output_txt.write(translated_text)\n",
    "\n",
    "# # Example usage\n",
    "# input_pdf_path = \"input.pdf\"  # Replace with the path to your input PDF file\n",
    "# output_txt_path = \"translated_output.txt\"  # Replace with the desired output text file path\n",
    "# target_language = \"en\"  # Replace with the target language code, e.g., \"fr\" for French\n",
    "\n",
    "# translate_pdf(input_pdf_path, output_txt_path, target_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a01807-b954-4a40-89e5-b9dc644d60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_pdf_path = \"./uploads/AFFAIRE C.P. ET M.N. c. FRANCE.pdf\"  # Replace with the path to your input PDF file\n",
    "output_pdf_path = \"frMNMTtranslated_output.txt\"  # Replace with the desired output PDF file path\n",
    "target_language = [\"en\"]  # Replace with the target language code, e.g., \"fr\" for French\n",
    "\n",
    "translate_pdf(input_pdf_path, output_pdf_path, target_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a69d4d4-45dc-4ca5-9916-483bab50e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "def create_pdf_from_text(input_txt_path, output_pdf_path):\n",
    "    doc = SimpleDocTemplate(output_pdf_path, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "\n",
    "    translated_text = \"\"\n",
    "    with open(input_txt_path, \"r\", encoding=\"utf-8\") as input_txt:\n",
    "        translated_text = input_txt.read()\n",
    "\n",
    "    story = []\n",
    "    paragraph = Paragraph(translated_text, styles[\"Normal\"])\n",
    "    story.append(paragraph)\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "# Example usage\n",
    "input_txt_path = \"frMNMTtranslated_output.txt\"  # Replace with the path to your translated text file\n",
    "output_pdf_path = \"frmmmttranslated_output.pdf\"  # Replace with the desired output PDF file path\n",
    "\n",
    "create_pdf_from_text(input_txt_path, output_pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77bf84-a187-43be-abea-a52addcc6ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
